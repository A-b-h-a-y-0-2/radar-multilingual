{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are taking samples from french wikipedia articles as our human corpus, and generating Ai text corpus using vicuna for text generation in french given starting words from a human text sample.\n",
    "\n",
    "- Now we have human Courpus and AI corpus in French \n",
    "\n",
    "- tested RADAR, Roberta on these samples \n",
    "\n",
    "- We translate the human and ai corpus to generate translated english human and translated english ai corpus using NMT\n",
    "\n",
    "- tested RADAR, Roberta on these samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btech/harshit.singh/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.1\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.1\")\n",
    "datawiki_fr  = load_dataset('wikipedia', '20220301.fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please slect appropriate device here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taking 1000 samples from wiki french articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_fr = []\n",
    "for i in range(1000):\n",
    "    human_fr.append(datawiki_fr['train'][i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.truncation_side = 'right'\n",
    "instruction=\"Please complete the given text in the french language : \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gnerating a French Ai text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_text_fr =[]\n",
    "count = 0\n",
    "for item in human_fr:\n",
    "    prefix_input_ids=tokenizer([f\"{instruction} {item}\"],max_length=30,padding='max_length',truncation=True,return_tensors=\"pt\")\n",
    "    prefix_input_ids={k:v.to(\"cpu\") for k,v in prefix_input_ids.items()}\n",
    "    outputs = model.generate(\n",
    "        **prefix_input_ids,\n",
    "        max_new_tokens = 512,\n",
    "        do_sample = True,\n",
    "        temperature = 0.6,\n",
    "        top_p = 0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    AI_texts=[\n",
    "        item.replace(\"Please complete the given text in the french language : \",\"\") for item in output_text\n",
    "    ]\n",
    "    ai_text_fr.append(AI_texts)\n",
    "    count+=1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the ai_text_corpus for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_fr = \"ai_text_fr2.txt\"\n",
    "with open(file_fr,'a+') as f:\n",
    "    for text in ai_text_fr:\n",
    "        f.write('start : '+str(text)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RADAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"# example: cuda:0\n",
    "detector_path_or_id = \"TrustSafeAI/RADAR-Vicuna-7B\"\n",
    "detector = transformers.AutoModelForSequenceClassification.from_pretrained(detector_path_or_id)\n",
    "detector_tokenizer = transformers.AutoTokenizer.from_pretrained(detector_path_or_id)\n",
    "detector.eval()\n",
    "detector.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAi's Roberta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"openai-community/roberta-large-openai-detector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dectecting over Original French texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_input = human_fr\n",
    "error = 0\n",
    "output_probs_list_fr=[]\n",
    "with torch.no_grad():\n",
    "  for i in Text_input:\n",
    "    inputs = tokenizer(i, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "    output_probs = F.log_softmax(detector(**inputs).logits,-1)[:,0].exp().tolist()\n",
    "    output_probs_list_fr.extend(output_probs)\n",
    "\n",
    "print(len(output_probs_list_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = human_fr\n",
    "prediction =[]\n",
    "for text in input:\n",
    "    text = text[:512]\n",
    "    prediction.append(pipe(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_h_pred = \"human_fr_human_pred.csv\"\n",
    "with open(file_h_pred,'a+') as f:\n",
    "    for text in output_probs_list_fr:\n",
    "        f.write(str(text[0])+'\\n')\n",
    "file_h_roberta_pred = \"human_fr_human_roberta_pred.csv\"\n",
    "with open(file_h_roberta_pred,'a+') as f:\n",
    "    for text in prediction:\n",
    "        f.write(str(text[0])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting over AI Generated French articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_input = ai_text_fr\n",
    "output_probs_list_fr_ai=[]\n",
    "with torch.no_grad():\n",
    "  for i  in Text_input:\n",
    "    inputs = detector_tokenizer(i, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "    output_probs = F.log_softmax(detector(**inputs).logits,-1)[:,0].exp().tolist()\n",
    "    output_probs_list_fr_ai.append(output_probs)\n",
    "\n",
    "print(len(output_probs_list_fr_ai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ai_text_fr\n",
    "prediction =[]\n",
    "for text in input:\n",
    "    text = text[:512]\n",
    "    prediction.append(pipe(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ai_pred = \"human_fr_ai_pred.csv\"\n",
    "with open(file_ai_pred,'a+') as f:\n",
    "    for text in output_probs_list_fr_ai:\n",
    "        f.write(str(text[0])+'\\n')\n",
    "file_ai_roberta_pred = \"human_fr_ai_roberta_pred.csv\"\n",
    "with open(file_ai_roberta_pred,'a+') as f:\n",
    "    for text in prediction:\n",
    "        f.write(str(text[0])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Translations to detect over English articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have currently used Opus-MT it is used as base model in easyNMT paper link : https://aclanthology.org/2020.eamt-1.61/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator_tokenizer = transformers.AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_fr_tr=[]\n",
    "for text in human_fr:\n",
    "    inp = text\n",
    "    inp = inp[:512]\n",
    "    input_ids = tokenizer(inp, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
    "    human_fr_tr.append(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_fr_tr = []\n",
    "for text in ai_text_fr:\n",
    "    inp = text\n",
    "    inp = inp[:512]\n",
    "    input_ids = tokenizer(inp, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
    "    ai_fr_tr.append(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection over Non-english Texts after translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_input = human_fr_tr\n",
    "output_probs_list_fr_tr_human=[]\n",
    "with torch.no_grad():\n",
    "  for i  in Text_input:\n",
    "    inputs = detector_tokenizer(i, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "    output_probs = F.log_softmax(detector(**inputs).logits,-1)[:,0].exp().tolist()\n",
    "    output_probs_list_fr_tr_human.append(output_probs)\n",
    "print(len(output_probs_list_fr_tr_human))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = human_fr_tr\n",
    "prediction =[]\n",
    "for text in input:\n",
    "    text = text[:512]\n",
    "    prediction.append(pipe(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_h_tr_pred = \"human_fr_human_tr_pred.csv\"\n",
    "with open(file_h_pred,'a+') as f:\n",
    "    for text in output_probs_list_fr_tr_human:\n",
    "        f.write(str(text[0])+'\\n')\n",
    "file_h_roberta_tr_pred = \"human_fr_human_roberta_tr_pred.csv\"\n",
    "with open(file_h_roberta_tr_pred,'a+') as f:\n",
    "    for text in prediction:\n",
    "        f.write(str(text[0])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_input = ai_fr_tr\n",
    "output_probs_list_fr_tr_ai=[]\n",
    "with torch.no_grad():\n",
    "  for i  in Text_input:\n",
    "    inputs = detector_tokenizer(i, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "    output_probs = F.log_softmax(detector(**inputs).logits,-1)[:,0].exp().tolist()\n",
    "    output_probs_list_fr_tr_ai.append(output_probs)\n",
    "print(len(output_probs_list_fr_tr_ai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ai_fr_tr\n",
    "prediction =[]\n",
    "for text in input:\n",
    "    text = text[:512]\n",
    "    prediction.append(pipe(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ai_tr_pred = \"human_fr_ai_tr_pred.csv\"\n",
    "with open(file_ai_pred,'a+') as f:\n",
    "    for text in output_probs_list_fr_tr_ai:\n",
    "        f.write(str(text[0])+'\\n')\n",
    "\n",
    "file_ai_roberta_tr_pred = \"human_fr_ai_roberta_tr_pred.csv\"\n",
    "with open(file_ai_roberta_tr_pred,'a+') as f:\n",
    "    for text in prediction:\n",
    "        f.write(str(text[0])+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
