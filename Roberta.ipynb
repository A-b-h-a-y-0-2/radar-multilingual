{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/roberta-large-openai-detector\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"openai-community/roberta-large-openai-detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0540, -0.7189]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai-community/roberta-large-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"openai-community/roberta-large-openai-detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.6516301035881042}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0540, -0.7189]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rtx4090/miniconda3/envs/ADS/lib/python3.11/site-packages/datasets/load.py:1461: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "datawiki_it = load_dataset('wikipedia', '20220301.it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1743035"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datawiki_it['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_it =[]\n",
    "for i in range(1000):\n",
    "    human_it.append(datawiki_it['train'][i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "human_it_tr = []\n",
    "with open('human_it_tr.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        human_it_tr.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "human_de_tr = []\n",
    "with open('human_de_tr.csv','r') as f:\n",
    "    reader = csv.reader(f,delimiter=',')\n",
    "    for row in reader:\n",
    "        human_de_tr.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alan Smithee stands as a pseudonym for a fictional director who is responsible for films in which the actual director does not want to have his name associated with the work. From 1968 to 2000 it was recommended by the Directors Guild of America (DGA) for such situations. Alternative writing methods are among others the original variant Allen Smithee as well as Alan Smythee and Adam Smithee. Also two partly Asian-looking writing methods Alan Smi Thee and Sumishii Aran belong – so',\n",
       " 'Actinium is a radioactive chemical element with the element symbol Ac and the atomic number 89. In the periodic table of the elements it stands in the 3rd IUPAC group, the scandium group. The element is a metal and belongs to the 7th period, d-block. It is the name giver of the group of Actinoids, of the following 14 elements. History The Actinium was discovered in 1899 by the French chemist André-Louis Debierne, who isolated it from pitchblende and initially resembled it with the Titan o',\n",
       " 'Ang Lee (; born 23 October 1954 in Chaozhou, Pingtung County, Taiwan) is a Taiwanese film director, screenwriter and producer. He is known as an award-winning director for such different films as Eat Drink Man Woman, Jane-Austen-Adaption Sinn und Senslichkeit und den Martial Arts-Film Tiger and Dragon. For his films Brokeback Mountain (2005) and Life of Pi: Schiffbruch mit Tiger (2012) he was awarded the Oscar in the category Best Director.',\n",
       " 'In sociology, connection is a technical term from the system theory of Niklas Luhmann and describes the selection that follows in a social encounter on a selection of the other side. These selections relate to each other. The ability to connect is the capacity of systems to ensure that further can be connected to the selections of a system. All social systems reproduce themselves via communication (e.g. economic system or politics) or actions',\n",
       " 'The logic of statements is a sub-area of logic that deals with statements and their connection by junktors, starting from structurally unstructured elementary statements (atoms), to which a truth value is assigned. In the classical logic of statements, each statement is assigned exactly one of the two truth values \"true\" and \"false\". The truth value of a compound statement can be determined without additional information from the truth values of its partial statements.',\n",
       " 'Aa Bertus Aafjes (1914–1993), NL Jeppe Aakjær (1866–1930), DK Johannes Aal (around 1500–1551), CH Hans Aanrud (1863–1953), NO Emil Aarestrup (1800–1856), DK Soazig Aaron (born 1949), FR Ivar Aasen (1813–1896), NO Ab Petrus Abelardus (1079–1142), FR Sait Faik Abasıyanık (1906–1954), TR Lynn Abbey (born 1948), US Jacob Abbott (1803–1879), US John Stevens Cabot Abbott (1805–1877), US Abdullah bin Abdul Kadir (1795–1852), MAL Abe Kōbō (1924–1993), JP Rebecca Abe (born 1967), D Hans Karl Abel (1876–1951)',\n",
       " 'Ha Haa–Han Ha Song-ran (1967) Alban Haas (1877–1968) Rudolf Haas (1877–1943) Wolf Haas (1960) Hella Haasse (1918–2011) Paavo Haavikko (1931–2008) Hans Habe (1911–1977) Martin Haberer Jürgen Habermas (1929) Emil Habibi (1922–1996) Marilyn Hacker (born 1942) Erich Hackl (1954) Friedrich Wilhelm Hackländer (1816–1877) Peter Hacks (1928–2003) Maja Haderlap (1961) Pierre Hadot (1922–2010) Gabriele Haefs (1953) Gisbert Haefs (1950) Hanswilhelm Haefs (1935–2015) Friedrich von Hagedorn (1708–1754) Rudolf Hagelstan',\n",
       " 'Ca Cab–Cap Fernán Caballero (1796–1877), ES George Washington Cable (1844–1925), US Guillermo Cabrera Infante (1929–2005), CU Pino Cacucci (* 1955), IT Caedmon (7th century), GB James M. Cain (1882–1977), US Rachel Caine (1962–2020), US Alice Calaprice (* 1941), US Moyra Caldecott (1927–2015), ZA Nigel Calder (1931–2014), GB Pedro Calderón de la Barca (1600–1681), ES Erskine Caldwell (1903–1987), US Taylor Caldwell (1900–1985), US Noël Calef (1907–1968), FR Andrew Calimach (* 1953), USA/RUM N',\n",
       " 'I Yi I (1536–1584) Ia Iacopone da Todi (1230/36–1306) Karl Iagnemma (1972) Iambulos Ib Alfred Ibach (1902–1948) Jorge Ibargüengoitia (1928–1983) Eva Ibbotson (1925–2010) Muhammad ibn Ammar (1031–1086) Ibn Battuta (1304–1368) Abbas Ibn Firnas (-888) Usama Ibn Munqidh (1095–1188) Ahmad ibn Abdallah ibn Zaidun (1003–1071) Henrik Ibsen (1828–1906) Ibuse Masuji (1898–1993) Ibykos (around 530 BC) Ic Jorge Icaza (1906–1978) Iceberg Slim (1918–1992) Id Robert Ide (1975) Wilhelm Ide (1',\n",
       " 'Ka Dieter B. Kabus (1941–1993) Ismail Kadare (b. 1936) Lamya Kaddor (b. 1978) Wolfgang Kaes (b. 1958) Franz Kafka (1883–1924) Donald Kagan (1932–2021) Janet Kagan (1946–2008) Jerome Kagan (1929–2021) Hans Kägi (1889–1971) Wolfgang Kahl (b. 1951) Heinz Kahlau (1931–2012) Hilde Köhler-Timm (b. 1947) Georg Kaiser (1878–1945) Joachim Kaiser (1928–2017) Ulrich Kaiser (1934–2015) Masha Kaléko (1907–1975) Ludwig Kalisch (1814–1882) Dieter Kalka (b. 1957) Tuula Kallioniemi (b. 1951) Karin Kallmaker (b. 1960) Fritz Kalm',\n",
       " 'Ja Edmond Jabès (1912–1991), FR Eberhard Jäckel (1929–2017), D Lisa Jackson (born 1952), U.S. Shirley Jackson (1916–1965), U.S. Heinrich Eduard Jacob (1889–1967), D/ US Uwe Jacobi (1939–2020), D Ludwig Jacobowski (1868–1900), D A. J. Jacobs (born 1968), U.S. Emil Jacobsen (1836–1911), D Jens Peter Jacobsen (1847–1885), DK Howard Jacobson (born 1942), GB Russell Jacoby (born 1945), U.S. Brian Jacques (1939–2011), GB Norbert Jacques (1880–1954), LU/ D Urs Jaeggi (1931–2021), CH/D Rona Jaffe (1931–2005), U.S. Markus Jäger (born 197)',\n",
       " 'Va Emile Mario Vacano (1840–1892), AU Andrew Vachss (1942–2021), US Klaus Vack (1935–2019), D Urvashi Vaid (born 1958), US Roger Vailland (1907–1965), FR Catherynne M. Valente (born 1979), USA Karl Valentin (1882–1948), D Thomas Valentin (1922–1980), D Valérie Valère (1961–1982), FR Paul Valéry (1871–1945), FR Günter Vallaster (born 1968), AT Jaume Vallcorba Plana (1949–2014), ES César Vallejo (1892–1938), PER Fernando Vallejo (born 1942), MEX Carlos G. Vallés (1925–2020), ES/IND Jules Vallès (1832–1',\n",
       " 'Ga Jostein Gaarder (1952) Diana Gabaldon (1952) Hans-Georg Gadamer (1900–2002) Carlo Emilio Gadda (1893–1973) William Gaddis (1922–1998) Rebecca Gablé (1964) Friedrich von Gagern (1882–1947) Zsuzsanna Gahse (1946) Gerd Gaiser (1908–1976) Patrick Gale (born 1962) Eduardo Galeano (1940–2015) Damon Galgut (born 1963) Mavis Gallant (1922–2014) Pierre Marie Gallois (1911–2010) John Galsworthy (1867–1933) Bettina Galvagni (1976) Patrick Galvin (1927–2011) Joshua Gamson (born 1962) Ludwig Ganghofer (1855–1920) Hans Ganz',\n",
       " 'Wa Edmund de Waal (born 1964) Wilhelm Heinrich Wackenroder (1773–1798) Stephan Wackwitz (born 1952) Martin Waddell (born 1941) Henry Wade (1887–1969) Stephan Waetzoldt (1849–1904) Stephan Waetzoldt (1920–2008) Wilhelm Waetzoldt (1880–1945) Klaus Wagenbach (1930–2021) Benjamin Wagner (born 1990) Bernd Wagner (born 1948) Christian Wagner (1835–1918) David Wagner (born 1971) Gerhard Wagner (1950–2011) Gottlieb Friedrich Wagner (1774–1839) Antje Wagner (born 1974) Jan Wagner (born 1971) S. O. Wagner (1902–1975) Mats Wahl (born 1945) P',\n",
       " 'Ba Baa–Ban Johannes Baader (1875–1955), D Albert Cornelis Baantjer (1923–2010), NL Johannes Baargeld (1892–1927), D Isaac Babel (1894–1941), RUS Jörg Baberowski (born 1961), D Ingrid Bachér (born 1930), D Guido Bachmann (1940–2003), CH Ingeborg Bachmann (1926–1973), AT Johann Friedrich Bachstrom (1686–1742) Albert Bächtold (1891–1981), CH Francis Bacon (1561–1626), GB Krzysztof Kamil Baczyński (1921–1944), PL Bae Su-ah (born 1965), ROK Julio Baghy (1891–1967), HU Enid Bagnold (1889–1981), GB Moham',\n",
       " 'Since Maria Dąbrowska (1889–1965), PL Simon Dach (1605–1659), D Robert Dachs (1955–2015), D / AT Anne Dacier (1654–1720), FR Didier Daeninckx (born 1949), FR Wolfgang Därr (born 1948), D Günther Däss (born 1926), D Adelheid Dahimène (1956–2010), AT Roald Dahl (1916–1990), GB Robert Alan Dahl (1915–2014), US Daniela Dahn (born 1949), D Felix Dahn (1834–1912), D Ralf Dahrendorf (1929–2009), D Hans Daiber (1927–2013), D Wilfried Daim (1923–2016), AT György Dalos (born 1943), HU Roque Dalton (1935–1975), ES',\n",
       " 'Sa Ferdinand von Saar (1833–1906) Umberto Saba (1883–1957) Robert Sabatier (1923–2012) Ernesto Sabato (1911–2011) Martin Sabrow (b. 1954) Mário de Sá-Carneiro (1890–1916) Leopold von Sacher-Masoch (1836–1895) Hans Sachs (1494–1576) Nelly Sachs (1891–1970) Jonathan Sacks (1948–2020) Oliver Sacks (1933–2015) Vita Sackville-West (1892–1962) Donatien-Alphonse-François, Marquis de Sade (1740–1814) Abu',\n",
       " 'Ta George Tabori (1914–2007) Antonio Tabucchi (1943–2012) Abdellah Taïa (born 1973) Paco Ignacio Taibo I (1924–2008) Paco Ignacio Taibo II (born 1949) Mariko Tamaki (born 1975) Susanna Tamaro (born 1957) Ulkü Tamer (1937–2018) Oğuz Tansel (1915–1994) Tristan Taormino (born 1971) Jean Tardieu (1903–1995) Rudolf Tarnow (1867–1933) Donna Tartt (born 1963) Jean-Baptiste Tati Loutard (1938–2009) Michel Tauriac (1927–2013) Kressmann Taylor (1903–1996) Te Michelle Tea (born 1971) Hernando Téllez (1908–1966) Uwe',\n",
       " 'Ma Amin Maalouf (born 1949) Michael Maar (born 1960) Paul Maar (born 1937) Rozena Maart (born 1962) Walter Maas (1900–1981) Joachim Maass (1901–1972) Donagh MacDonagh (1912–1968) George MacDonald (1824–1905) Marianne MacDonald (born 1934) Ross Macdonald (1915–1983) Niccolò Machiavelli (1469–1527) John Henry Mackay (1864–1933) Compton Mackenzie (1883–1972) Bernard MacLaverty (born 1942) Archibald MacLeish (1892–1982) Charlotte MacLeod (1922–2005) Ian R. MacLeod (born 1956) Ken MacLeod (born 1954) Louis MacNeic',\n",
       " 'Oa Joyce Carol Oates (b. 1938) Ob René Oberholzer (b. 1963) Urs Oberlin (1919–2008) Edna O suchBrien (b. 1930) Fitz-James O suchBrien (1828–1862) Flann O suchBrien (1911–1966) Kate O suchBrien (1897–1974) Michael O suchBrien (1948–2015) Oc Máirtín Ó Cadhain (1906–1970) Sean O suchy (1880–1964) Carol O sucheconnell (b. 1947) Frank O suchconnor (1903–1966) Gemma O suchconnor (b. 1940) Joseph O suchconnor (b. 1963) Mary Flannery O such connor (1925–1964) Tomás Ó Criomhthain (1856–1937) Od Peter O sucha Joyce Carol Oates (1920–2010) Oe Kenzaburō',\n",
       " 'Fa Kurt Faber (1883–1929), D Jan Fabricius (1871–1964), NL Johann Albert Fabricius (1668–1736), D Johan Fabricius (1899–1981), NL Lillian Faderman (born 1940), US Peter Faecke (1940–2014), D Sait Faik (1906–1954), TR Colin Falconer (born 1953), GB Knut Faldbakken (born 1941), NO Giorgio Faletti (1950–2014), IT Gustav Falke (1853–1916), D Konrad Falke (1880–1942), CH Oriana Fallaci (1929–2006), IT Hans Fallada (1893–1947), D György Faludy (1910–2006), HU Alfred Fankhauser (1890–1973), CH John Fant',\n",
       " 'Ea Charles Eastman (1858–1939), (Indian) David Easton (1917–2014), CA Eb Martin Ebbertz (born 1962), D Isabelle Eberhardt (1877–1904), RUS / CH Georg Ebers (1837–1898), D David Ebershoff (born 1969), US Roger Ebert (1942–2013), US Robert Ebner (1940–2008), D Marie von Ebner-Eschenbach (1830–1916), AT Ec José Maria Eça de Queiroz (1845–1900), PT Jean Echenoz (born 1947), FR Dietrich Eckart (1868–1923), D Wolfgang U. Eckart (1952–2021), D Fritz Eckega (born 1955), D Christopher Ecker (born 1967), D',\n",
       " 'La Jacques Lacan (1901–1981), FR Alberto de Lacerda (1928–2007), PT, GB Mercedes Lackey (born 1950), USA Stephan Lackner (1910–2000) Ernesto Laclau (1935–2014) Erwin Lademann (1923–2015), D Oliver La Farge (1901–1963), USA Celso Lafer (born 1941) Dany Laferrière (born 1953) Raphael Aloysius Lafferty (1914–2002), USA Carmen Laforet (1921–2004) Jean-Luc Lagarce (1957–1995), FR Geoffroy de Lagasnerie (born 1981), FR Pär Lagerkvist (1891–1974), SE Selma Lagerlöf (1858–1940), SE Ronald D. Laing (1927–1989)',\n",
       " 'Na Magdalen Nabb (1947–2007) Vladimir Nabokov (1899–1977) Maurice Nadeau (1911–2013) Melinda Nadj Abonji (born 1968) Karl Gottfried Nadler (1809–1849) Isabella Nadolny (1917–2004) Sten Nadolny (born 1942) Kiran Nagarkar (1942–2019) Ivan Nagel (1931–2012) Mirosław Namacz (1984–2007) V. S. Naipaul (1932–2018) Salah Naoura (born 1964) Donna Jo Napoli (born 1948) R. K. Narayan (1906–2001) Wolf-Dieter Narr (1937–2019) Armin Nassehi (born 1960) Peter Nathschläger (born 1965) Hans Joachim Nauschütz (1940–2003)',\n",
       " 'Pa Gert von Paczensky (1925–2014), German journalist and writer Susanne von Paczensky (1923–2010), German non-fiction author Krzysztof Paczuski (1956–2004), Polish poet Tarcísio Padilha (1928–2021), Brazilian philosopher Heberto Padilla (1932–2000), Cuban poet Elifius Paffrath (1942–2016), German theatre scholar, playwright and radio playwright Camille Paglia (born 1947), American cultural scholar and author Marcel Pagnol (1895–1974), French schrif',\n",
       " 'Qa Nizar Qabbani (1923–1998), SYR Qi Qiu Xiaolong (born 1953), CHN / USA Qu Helmut Qualtinger (1928–1986), AT David Quammen (born 1948), US Pier Antonio Quarantotti Gambini (1910–1965), IT Salvatore Quasimodo (1901–1968), IT Carol Queen (born 1957), USA Ellery Queen (collective pseudonym; 1905–1971 and 1905–1982), US Jorge H. Queirolo (born 1963), ECU Raymond Queneau (1903–1976), FR William Quindt (1898–1969), D Daniel Quinn (1935–2018), US Lawrence J. Quirk (1923–2014), US Hermann Quistorf (1884–1969), D Sabrina Q',\n",
       " 'Ra Wilhelm Raabe (1831–1910) François Rabelais (1494–1553) Jean Racine (1639–1699) Ann Radcliffe (1764–1823) Fritz J. Raddatz (1931–2015) Hartmut Radebold (1935–2021) Cay Rademacher (born 1965) Raymond Radiguet (1903–1923) Iris Radisch (born 1959) Friedrich Radszuweit (1876–1932) Peter Radtke (1943–2020) Marc-André Raffalovich (1864–1934) Atiq Rahimi (born 1962) Fred Rai (1941–2015) Ferdinand Raimund (1790–1834) Werner Raith (1940–2001) Hannu Raittila (born 1956) Hannu Rajaniemi (born 1978) David Rakoff (1964–2012) Tar',\n",
       " 'U Anneliese Ude-Pestel (1921–2017), D Milan Uhde (born 1936), CZ Ludwig Uhland (1787–1862), D Marie Ulfers (1888–1960), D Udo Ulfkotte (1960–2017), D Arne Ulbricht (born 1972), D Anya Ulinich (born 1973), U.S. Lyudmila Ulizkaja (born 1943), RU Otto Ullrich (1938–2015), D Elisabeth von Ulmann (Elisabeth Meyer-Runge; 1929–2005), D Karl Heinrich Ulrichs (1825–1895), D Ahmet Ümit (born 1960), TR Mehmet Ünal (born 1951), TR Giuseppe Ungaretti (1888–1970), IT Gert Fritz Unger (1921–2005), D Tomi Ungerer (1931–2019)',\n",
       " 'Y Irvin D. Yalom (born 1931), US Yang Gui-ja (born 1955), ROK Richard Yates (1926–1992), US W. Edgar Yates (1938–2021), GB William Butler Yeats (1865–1939), IRL Frank Yerby (1916–1991), US Yi In-seong (born 1953), ROK Carol Beach York (1928–2013), US Banana Yoshimoto (born 1964), JP Patricia Young (born 1954), CAN Marguerite Yourcenar (1903–1987), BE/FR Yun Dae-nyong (born 1962), ROK Yun Heung-gil (born 1942), ROK Yun Hu-myeong (born 1946), ROK Yannic Broxtermann (born 2001), DEU Y',\n",
       " 'Za Franciszek Zabłocki (1752–1821) Jan Zábrana (1931–1984) Manfred Zach (born 1947), D Justus Friedrich Wilhelm Zachariae (1726–1777), D Gerd Zacher (1929–2014), D Christina Zacher (born 1954), D Adam Zagajewski (1945–2021), PL Peter-Paul Zahl (1944–2011), D Ernst Zahn (1867–1952) Johannes Christoph Andreas Zahn (1817–1895) Johann Zahn (1641–1707) Leopold Zahn (1890–1970) Timothy Zahn (born 1951) Daniel Zahno (born 1963) Norbert Zähringer (born 1967) Heinz Zahrnt (1915–2003) Amir Zaidan (born 1964) Dschu',\n",
       " 'Anthony Minghella, CBE (born 6 January 1954 on the Isle of Wight; died 18 March 2008 in London) was a British film director, film producer, screenwriter, playwright, radio playwright, theatre and opera director. Life Minghella was the son of Italian-Scottish parents, who operated an ice cream factory on the Isle of Wight. After graduating from school, he studied at the University of Hull, where he worked for a time as a lecturer. 1978 he made a first short film.',\n",
       " 'The history of U.S. film is a chapter in film history that is relevant to both film art and the economy of film precisely because of the prominent position of the United States as a film nation. Hollywood, a district of Los Angeles, gained world fame as the center of the U.S. film industry, which is why the name is often synonymous with the entire American film industry.',\n",
       " 'Planes for units of measurement, unit resolutions, unit prefixes or short prefixes or resolutions are used to form multiples or parts of units of measurement to avoid numbers with many digits. SI prefixes SI prefixes are decimal prefixes defined for use in the International System of Units (SI). They are based on ten potentials with integer exponents. One distinguishes between the name of the prefix and its symbol. The symbols are internationally uniform. The names distinguish',\n",
       " 'Many, but not all, authors abbreviate without point and spaces: For example, the abbreviature is usually debreviated with idR. Regardless of this, acronyms, as usual, are formed without points. The following list of abbreviations from the legal language only tries to list correct abbreviatures, i.e. with point and spaces, unless the ruffled spelling has found its way into everyday life and dominates there (e.g. eG, GbR or MdB) or the Abbrevi',\n",
       " 'This is a list of technical abbreviations used in the IT field. A B C D E F G H I J K L M O P Q R S T U V W X Y Z See also List of abbreviations (net jargon) List of filename extensions Web links Page for searching abbreviations 14000 Abbreviations from computer science and telecommunications Page for searching abbreviations (in English) Abbreviations (computers) Computer',\n",
       " 'This is a list of companies (names of companies) and their origins (etymology). Often the companies are abbreviations or acronyms of the names of the founders, previous companies or the company\\'s target. # 20th Century Fox — formed in 1935 by the merger of Twentieth Century Pictures and the Fox Film Corporation of William Fox. 3B Scientific – Best Quality, Best Value, Best Service 3M — The Minnesota Mining and Manufacturing Company (for example, \"Mining and Manufacturing Company Minnesota\").',\n",
       " 'ISO 4217 is the standard for currency short characters published by the International Organization for Standardization, which is to be used in international payments for unambiguous identification. On 1 August 2015 the new version ISO 4217:2015 was published. This 8th version replaces the predecessor from 2008. Classification Alphabetical Codes The abbreviations each include three letters. The first two are usually the country identification according to ISO 3166-1 ALPHA-2 (for example A',\n",
       " 'An axis jump is a film cut with which the relation axis of the figures or groups is skipped. Eye axes or relation axes between the actors and each other or the point of interest of the protagonist form a thought-provoking line. Projected on the screen, this line represents a \"left-right- and \"top-down relationship\" between the actors. A axis jump is a cut in which this relationship reverses.',\n",
       " 'Sir Alfred Joseph Hitchcock KBE (born August 13, 1899 in Leytonstone, England – April 29, 1980 in Los Angeles) was a British film director, screenwriter, film producer and film editor. He moved to the USA in 1939 and also took on US citizenship on April 20, 1955. Hitchcock is considered one of the most influential film directors in his style to this day. He established the terms Suspense and MacGuffin. His genre was the thriller, whose tension he used with humor',\n",
       " 'The Auteur theory (from frz. \"Auteur\" = author) is a film theory and the theoretical basis for the author\\'s film – in particular the French one – in the 1950s, which was delimited from the \"producer cinema\". Even today, the definition of the Auteur concept is constantly being further developed. At the heart of the film is the director\\'s or filmmaker\\'s auteur theory as the intellectual author and central designer of the work of art.',\n",
       " 'Aki Olavi Kaurismäki (born 4 April 1957 in Orimattila) is a multi-award-winning Finnish film director. Life and work Aki Kaurismäki studied literature and communication at the University of Tampere. Besides various auxiliary jobs, for example as a postman or in the catering industry, he was the editor of a university film magazine. In addition, he wrote film reviews for the magazine Filmihullu from 1979 to 1984. The first screenplay followed in 1980 for the medium-length film Der Liar (Vale)',\n",
       " 'Anime (jap., [], German frequently [], plural: Animes and Anime) refers to animated films and animated series produced in Japan. In Japan itself, Anime stands for all kinds of animated films and series, for which both domestically produced and imported. It forms the counterpart to the Manga, the Japanese comic. Japan has the most extensive animation culture worldwide. Definition and conceptual history In Japanese, \"Anime\" can refer to any animation film, both from the',\n",
       " 'The action film (by engl. action: act, plot, movement) is a genre of entertainment cinema, in which the progress of the external action is driven and illustrated by mostly spectacular staged fight and violence scenes. It is more about stimulating actions than about content connections, empathic co-experience of the emotional world of the protagonists or artistic-aesthetic image worlds. Main components of action films are therefore usually elaborately filmed stunts, close combat scenes,',\n",
       " 'Alfredo James (born April 25, 1940 in New York City) is an American actor, film director and film producer. Since the 1970s he has appeared in numerous film classics; he is regarded by many critics and viewers as an outstanding character actor of contemporary American film and theatre. During his career he has been awarded, among others, the Oscar, the Golden Globe Award, the Tony Award and the National Medal of Arts.',\n",
       " 'Alcohols () are organic chemical compounds which have one or more hydroxy groups (–O–H) bound to different aliphatic carbon atoms. The difference between alcohols and other compounds with OH groups (e.g. enoles, semiacetals or carboxylic acids) as part of the functional group is that in alcohols, each carbon atom carrying an OH group must be sp3-hybridised and, apart from the hydroxy group, only carbon or hydrogen atoms are bound.',\n",
       " 'The Visitors is the eighth studio album of the Swedish pop group ABBA. It was first released on November 30, 1981 in Sweden. Recordings in the studio lasted from March to November 1981. About at the same time as the LP, One of Us was released as a single and became the last international chart success of the group, while the following single release Head over Heels flopped. The Visitors was also the first released audio CD of the music history and was presented on August 17, 1982',\n",
       " \"Aluminium is a chemical element with the element symbol Al and the atomic number 13. In the periodic table, aluminium belongs to the third main group and to the 13th IUPAC group, the boron group, which was formerly called the group of earth metals. There are numerous aluminium compounds. Aluminium is a silvery white light metal. In the earth's shell, after oxygen and silicon, it is the third most common element and in the earth's crust the most common metal.\",\n",
       " 'Antimony (from Latin antimony, probably from Arabic \"al-ithmîd(un)\" (, antimonsulphide or stibnit)) is a chemical element with the element symbol Sb (of \"(grey) sphincter gloss\") and the order number 51. In the periodic table it stands in the 5th period and the 5th main group, respectively 15th IUPAC group or nitrogen group. In the stable modification it is a silver-glossing and brittle half-metal. Name, History It is believed that the name was traced back to the late Greek anthemon (\"blow\").',\n",
       " 'Argon is a chemical element with the symbol Ar (until 1957 only A) and the atomic number 18. In the periodic table it stands in the 8th main group or the 18th IUPAC group and is therefore one of the noble gases. Like the other noble gases it is a colourless, extremely reactionary, single-atomous gas. In many properties such as melting and boiling point or density it stands between the lighter neon and the heavier krypton. Argon is the most common noble gas occurring on earth, the proportion a',\n",
       " 'Arsenic [] is a chemical element with the element symbol A and the atomic number 33. In the periodic table of the elements it stands in the 4th period and the 5th main group, respectively 15th IUPAC group or nitrogen group. Arsenic is rarely dignified, mostly in the form of sulphides. It belongs to the semi-metals, as it shows metallic or non-metallic properties depending on modification. In colloquial terms, the arsenic known as murder poison is usually simply called \"arsenic\". Arsenic compounds are known as arsenic compounds.',\n",
       " 'Astat is a radioactive chemical element with the element symbol At and the atomic number 85. In the periodic table it stands in the 7th main group or the 17th IUPAC group and is therefore one of the halogens. Astat arises from the natural decay of uranium. Astat is the rarest naturally occurring element of the earth, which must be artificially produced if necessary. History When Dmitri Mendeleyev established his periodic table in 1869, he said the existence of some of these elements.',\n",
       " 'Alkali metals are called the chemical elements lithium, sodium, potassium, rubidium, caesium and francium from the 1st main group of the periodic table. They are silvery shiny, reactive metals, which have a single electron in their valence shell, which they can easily release as a strong reducing agent. Although hydrogen is also in the first main group in many representations of the periodic table – at the top and usually separated with a gap, or presented in a different color – k',\n",
       " 'Actinoids (-oeides) is a group name of certain similar elements. Actinium and the 14 elements in the periodic table are attributed to it: thorium, protactinium, uranium and the transuranes neptunium, plutonium, americium, curium, berkelium, california, Einsteinium, fermium, mendelevium, Nobelium and lawrencium. Actinium does not belong to the actinium-like elements, but the nomenclature of IUPAC here follows the following:',\n",
       " 'Americium () is a chemical element with the element symbol Am and the atomic number 95. In the periodic table it stands in the group of Actinoids (7th period, f-block) and is also one of the transuranes. Americium is beside Europium the only element named after a part of the earth. It is an easily deformable radioactive metal silvery-white appearance. Americium has no stable isotope. On earth it occurs exclusively in artificially produced form. The element was first introduced in late autumn.',\n",
       " 'Atoms (from \"indivisible\") are the building blocks, which consist of all solid, liquid or gaseous substances. All material properties of these substances as well as their behaviour in chemical reactions are determined by the properties and spatial arrangement of their atoms. Each atom belongs to a certain chemical element and forms its smallest unit. Currently 118 elements are known, of which about 90 occur naturally on Earth. Atoms of different elements differ in',\n",
       " 'A doctor or a doctor is a medically trained person authorised to practise medicine. The medical profession applies to prevention (prevention), detection (diagnosis), treatment (therapy) and aftercare of diseases, suffering or health impairments and also includes training activities. Doctors are at the service of health and are committed to moral and ethical principles in their actions (see, for example, the Geneva Declaration of the World Medical Association).',\n",
       " \"Anthropology (in the 16th century as an anthropologia formed from, and -logy: human science, teaching of man) is the science of man. It is understood in the German language area and in many European countries primarily as a natural science. After Charles Darwin's theory of evolution, natural or physical anthropology considers man as a biological being.\",\n",
       " 'Alexander the Great (also known as Alexander III of Macedonia) (c. 20 July 356 BC in Pella – 10 June 323 BC in Babylon) was King of Macedonia and Hegemon of the Corinthian Covenant from 336 BC to his death. Alexander stretched the boundaries of the empire that his father Philip II had built from the formerly rather insignificant small state of Macedonia and several Greek Poleis, through the so-called Alexander procession and the conquest of the Achaemenid Empire to the Indian Empire.',\n",
       " 'The Antiquity (from ) was an era in the Mediterranean, which ranged from 800 B.C. to about 600 A.D., although its beginning is sometimes still much earlier. Classical antiquity differs from previous and subsequent epochs by common and consistent cultural traditions, whose influence is characteristic in many thematic areas up to modern times. It covers the history of ancient Greece, Hellenism and the Roman Empire. The Roman Empire united',\n",
       " 'Anthony Hope was the pseudonym of Sir Anthony Hope Hawkins (b. February 9, 1863 in London; † July 8, 1933 in Walton-on-the-Hill, Surrey), a British lawyer and writer. Anthony Hope was a son of Reverend Edward Connerford Hawkins, an Anglican clergyman, and Jane Isabella Grahame. He left Oxford University in 1885 with a first-class degree and became a lawyer in London. He married in 1903, had two sons and a daughter.',\n",
       " 'The Ångström [] (according to Swedish physicist Anders Jonas Ångström) is a unit of measure of length. The unit sign is Å (A with ring). An Ångström corresponds to the ten millionth part of a millimeter. The Ångström is not a SI unit. 1 Å = 100 pm = 0.1 nm = 10−10 m The Ångström is used especially in crystallography and chemistry to work with \"simple\" numerical values. Thus, 1 Å is the typical order of magnitude for atomic radii and distances of atoms in crystal structures.',\n",
       " 'The ampere [] with unit character A, named after the French mathematician and physicist André-Marie Ampère, is the SI base unit of the electrical current and at the same time SI unit of the derived size magnetic flow. Although the surname of the name giver Ampère with Gravis is written, the SI unit is written in the German and English language space without accent, i.e. „Ampere\". Definition An ampere corresponds to a flow of 1 coulomb (C) per second through the conductor\\'s cross',\n",
       " 'Acre (Plural english acre or acre;, plural acres) is an Anglo-American unit of measurement from the British Isles for the area determination of land and corresponds in the metric system exactly 4046,8564224 m2 or roughly 4047 m2 or 0.4 ha respectively. This unit is abbreviated with ac. Typologically comparable units of measurement are the morning, the daily work, the yoke and the juchart. In the United States, the size of land for land and land measurements alone with the two',\n",
       " 'Apostilb (Unit sign asb) is an outdated unit of luminance of self-luminescent bodies. In 1942 Blondel (Unit sign: blondel) was proposed as another name, the name was chosen as a memorial to the French physicist André-Eugène Blondel. A use or official definition of this name is not currently detectable. Since 1978 the apostilb is no longer an official unit. It is a subunit of Stilb (Unit sign: ) and above it is linked to the Lambert (Ei',\n",
       " 'This or the Ar, in Switzerland the Are, is a unit of area in the metric system of 100 m2 with the unit sign a (often not or wrongly abbreviated: Ar or ar). 100 a give 1 ha. A square with the area content of 1 a has an edge length of ten meters, therefore one speaks also of a square decameter (dam2). The Ar is not a SI unit; in contrast to the hectare it is not even approved for use with the SI. In the EU and Switzerland the Ar or the Are is legal',\n",
       " 'Work is a purpose-conscious and socially supported by institutions (breaches) special form of activity with which people try to survive in their environment since their incarnation. On the anthropology of the \"work\" It is already disputed whether one can call targeted physical effort of animals (for example, the instinctive nest building or the addressed pulling of a plough) as \"work\". The philosophical anthropology usually assumes that \"work only in animal-humans\"',\n",
       " 'The atomic mass unit (unit sign: u for unified atomic mass unit) is a unit of mass. Its value is determined on the mass of an atom of the carbon isotope 12C. The atomic mass unit is approved for use with the International Unit System (SI) and a legal unit of measurement. It is used in the specification not only of atomic but also of molecular masses. In biochemistry, in the USA also in organic chemistry, the atomic mass unit is also called Dal',\n",
       " 'An anglicism is a linguistic expression which has been incorporated into another language by means of a copy from English. This can occur in all areas of a language system, from the sounding to the teaching of forms, syntax, semantics to vocabulary, as well as the areas of language usage and language level (technical language, everyday language, slang and other). If they have become an integral part of the borrowed language through regular use, or as a new meaning of a word or as a new language, they have become an integral part of the borrowed language.',\n",
       " \"An astronomer (from ástron ‘Stern, Gestirn' and nómos ‘Law') is a (mostly academically educated) person who deals scientifically with astronomy. Main activity of astronomers If one limits the term astronomer to those scientists who devote themselves mainly to astronomy, then usually two of the following activities are the subject of the profession: scientific research in the field of astronomy, especially in astrophysics, astrometry, cosmology or in the field of astronomy.\",\n",
       " 'Alan Mathison Turing OBE, FRS (* 23 June 1912 in London; † 7 June 1954 in Wilmslow, Cheshire) was a British logician, mathematician, cryptoanalyst and computer scientist. Today he is regarded as one of the most influential theorists of early computer development and computer science. Turing created a large part of the theoretical foundations for modern information and computer technology. His contributions to theoretical biology proved to be pioneering.',\n",
       " \"Archaeology (and λόγος lógos ‘Teaching'; literally, ‘Teaching of Antiquities') is a science that explores the cultural development of humanity through scientific and humanities methods. It has developed worldwide into a network of various theoretical and practical disciplines. Archaeology deals with material legacies of man, such as buildings, tools and works of art.\",\n",
       " 'The (ASCII, alternatively US-ASCII, often pronounced, ) is a 7-bit character encoding; it corresponds to the US variant of ISO 646 and serves as the basis for later character set encodings based on more bits. The ASCII code was first approved on 17 June 1963 by the American Standards Association (ASA) as the standard ASA X3.4-1963 and was substantially updated in 1967/1968 and most recently in 1986 (ANSI X3.4-1986) by its successor institutions and is still used today.',\n",
       " 'The outer band of the upper ankle is composed of three bands (the \"lateral band apparatus\"): Ligamentum fibulotalare anterius and posterius, as well as Ligamentum fibulocalcaneareare. When turning outwards (Supination trauma) there is usually a tear or tear of the Lig. fibulotalare anterius or/and the Lig. calcaneofibulare, less often is the complete rupture of all three bands. A rupture of at least one of these three bands is also called a fibular rupture.',\n",
       " 'An alphabet (early New High German from church Latin, from alphabētos) is the total of the smallest characters or letters of a language or several languages in a defined order. The letters can be linked to words via orthographic rules and thus represent the language in writing. The order of the letters specified in the alphabet allows the alphabetical sorting of words and names in dictionaries, for example.',\n",
       " 'Sir Arthur Travers Harris, 1st Baronet GCB OBE AFC, called Bomber-Harris, (born 13 April 1892 in Cheltenham; died 5 April 1984 in Goring-on-Thames) was a high-ranking officer of the Royal Air Force, most recently in the rank of Marshal of the Royal Air Force. During the Second World War he was commander-in-chief of the RAF Bomber Command from February 1942 and is one of the most controversial persons of the air war in World War II because of the area bombardments he ordered.',\n",
       " 'Arthur Wellesley, 1st Duke of Wellington (probably 1 May 1769 in Dublin, Ireland; † 14 September 1852 in Walmer Castle near Deal, Kent, England), was Field Marshal and the most outstanding British military leader of the Napoleonic period, as well as British foreign and two prime ministers. He won over Napoleon at the Battle of Waterloo. His life and childhood was from English-Irish nobility and was the third surviving son of Garret Wesley, 1st Earl of Mornington and Anne, who',\n",
       " 'Astronomy (from \"stars\" and \"law\") or starry science is the science of the stars. It uses scientific means to explore the positions, movements and properties of objects in the universe, i.e. the celestial bodies (planets, moons, asteroids, stars including the sun, star clusters, galaxies and galaxy clusters), interstellar matter and the radiation occurring in space. In addition, it strives for an understanding of the universe as a whole, its essence.',\n",
       " 'Angelina Jolie [], DCMG (born June 4, 1975 as Angelina Jolie Voight in Los Angeles, California) is an American actress, film director, film producer and screenwriter. During her marriage to Brad Pitt, she was named Angelina Jolie Pitt. She became internationally known with the portrayal of video game hero Lara Croft in Lara Croft: Tomb Raider (2001). She has had further commercial successes with the films Mr. & Mrs. Smith (2005), Wanted (2008), Salt (2010) and Maleficent – Die du',\n",
       " 'Archimedes of Syracuse (Greek: \"Archimedes of Syracuse\"; c. 287 B.C. presumably in Syracuse; † 212 B.C.) was a Greek mathematician, physicist and engineer. He is considered one of the most important mathematicians of antiquity. His works were also important in the development of higher analysis in the 16th and 17th centuries. Life About the life of Archimedes is little known and much is considered to be a legend. Archimedes, born about 287 B.C.',\n",
       " 'Aristotle (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle) (or Aristotle).',\n",
       " \"Abraham Lincoln (b. February 12, 1809 near Hodgenville, Hardin County, today: LaRue County, Kentucky; d. April 15, 1865 in Washington, D.C.) served as the 16th President of the United States of America from 1861 to 1865. In 1860 he was elected for the first time and succeeded in re-election for a second term in 1864. He was the first President of the Republican Party and the first to fall victim to an assassination. Lincoln's presidency is considered one of the most important in the history of the Vere\",\n",
       " 'Old female summer is the term for a meteorological singularity. It is a phase of even weather in autumn, often at the end of September and October, which is characterized by a stable high pressure area and a warm end to the summer.The short-dryer weather allows a good distance view, intensifies the foliage fall and the foliage discoloration. Word origin The origin of the word is not safe, especially since in addition to the term Old female summer also numerous other such as Erhnlsu',\n",
       " 'Only-Sultan (,, ) is the capital of Kazakhstan. It is located in the northern part of the country in the middle of the Kazakh steppe on the banks of the river Ishim. With inhabitants (as of ) it is the second largest city of the country after Almaty. The city has changed its name several times in the recent past. Thus it was called Akmolinsk until 1960, 1961 to 1991 Zelinograd, 1992 to 1998 Aqmola and most recently from 1998 to 2019 Astana. The city was founded in 1830 as a Russian fortress. In the 1950s Yah',\n",
       " 'Angela Dorothea Merkel (b. Kasner; * 17 July 1954 in Hamburg) is a German politician (CDU). She was Chancellor of the Federal Republic of Germany from 22 November 2005 to 8 December 2021. She is the eighth person, at the same time the first woman, the first person from East Germany and the first person who was born after the founding of the Federal Republic, who was elected to this office. From April 2000 to December 2018, she was Federal Chairman of the CDU. Merkel grew up in the GDR and was a physicist there.',\n",
       " \"Alicia Silverstone [] (born October 4, 1976 in San Francisco, California) is an American actress. In the 1990s she became famous for the film Clueless – What else! and Batgirl in Batman & Robin as well as for her appearances in three music videos by the band Aerosmith. Youth Silverstone was born as the daughter of British parents, a real estate investor and a flight attendant. She has two older siblings, a half sister from her father's first marriage and a sister from a\",\n",
       " \"Abu r-Raihan Muhammad b. Ahmad al-Bīrūnī (; Persian also only briefly, Abū Raiṭān Bīrūnī) – abbreviated also (al-)Biruni; born on September 4, 973 in the choresmic capital Kath (not far from today's Chiwa in Uzbekistan); died on December 9, 1048 in Ghazna (today in Afghanistan) was a Persian (choresmic) universal scholar, mathematician, cartographer, astronomer, astrologer, philosopher, pharmacologist, mineralogist, explorer, historian and translator in Central Asia.\",\n",
       " 'Ankara [], formerly Angora (ancient name, ), has been the capital of Turkey and the province of Ankara since 1923. According to Turkish law, the city, which is the largest city in Turkey (Büyükşehir Belediyesi) and is now identical to the province of the same name, had about 5.7 million inhabitants in 2021 and is therefore the second largest city in the country after Istanbul. Etymology and name of the city The exact etymological origin of the name Ankara is not known. Pausani',\n",
       " 'Atheism (from \"without God\") refers to the absence or rejection of faith in a God or God. In contrast, deism and theism (from \"God\") refer to faith in gods, whereby monotheism refers to faith in one God and polytheism to faith in several gods. Atheism in the wider sense also includes agnostic atheism, after which an existence of God or gods is unexplained or inexplicable. In the narrow sense, it refers to d',\n",
       " \"Anna Seghers (born 19 November 1900 in Mainz; died 1 June 1983 in East Berlin; born Annette (Netti) Reiling, married as Netty Radványi) was a German writer and from 1952 to 1978 president of the Writers' Association of the GDR. Life Origin and education Anna Seghers was the only child of the Mainz art and antique dealer Isidor Reiling and his wife Hedwig (born Fuld). Her maternal grandfather was the Frankfurt lawyer Salomon Fuld.\",\n",
       " 'The atmosphere [] (from,,,,,, and ) is the gas-shaped shell around larger celestial bodies – especially around stars and planets. It consists mostly of a mixture of gases that can be held by the gravity field of the celestial body. The atmosphere is the densest on the surface and passes smoothly into the interplanetary space at high altitudes. It determines in the case of its existence the appearance of a celestial body. The hot atmospheres of stars reach',\n",
       " \"Arvo Pärt is an Estonian composer who is considered to be one of the most important living composers of New Music. He has an Austrian citizenship. From 1981 to 2008 he lived in Berlin. Life Arvo Pärt's musical education began at the age of seven and at the age of fourteen he wrote his first own compositions. In 1954 he began studying music, worked as a sound engineer at the Estonian Hö\",\n",
       " \"Afghanistan (Pashtunian and ) is a landlocked state at the intersection of South Asia, Central Asia and Near Asia, bordering Iran, Turkmenistan, Uzbekistan, Tajikistan, the People's Republic of China and Pakistan. Three-quarters of the country consists of difficult-to-reach mountain regions. After the Soviet Union invaded in 1979 – financed by the United States and Saudi Arabia – Mujaheddin defeated the government supported by the Soviet Union.\",\n",
       " \"Atonal music generally refers to a music characterized by so-called atonality, which is based on the chromatic scale, whose harmonics and melodicity are not fixed to a tonal centre or a fundamental tone – in contrast to (major minor) tonality or modality. The term was initially applied in polemic intention by conservative music criticism to the compositions of the Vienna School, in particular to Arnold Schönberg's Three Piano Pieces op. 11 (1909), and was originally meh\",\n",
       " 'Aquilegia is a genus of plants in the family Ranunculaceae. The 70 to 75 species are mainly found in the temperate areas of the northern hemisphere. Varieties of some Aquilegia species are used as ornamental plants. Description Vegetative characteristics Akelei species are perennial (usually three to five-year-old) to perennial herbaceous plants. The richly branched root system forms slender, slightly woody rhizomes with a permanent stem root as a practice.',\n",
       " 'Aleister Crowley (born October 12, 1875 as Edward Alexander Crowley in Leamington Spa, died December 1, 1947 in Hastings, East Sussex) was a British occultist, writer and mountaineer. Crowley described himself as the Great Animal 666. From 1898 to 1900 he was a member of the Hermetic Order of the Golden Dawn, after which he founded his own societies, partly based on the concepts of the Golden Dawn. In 1904 he wrote the book Liber AL vel Legis (\"Book of the Law\").']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_de_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m \u001b[39minput\u001b[39m:\n\u001b[1;32m      5\u001b[0m     text \u001b[39m=\u001b[39m text[:\u001b[39m512\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m     prediction\u001b[39m.\u001b[39mappend(pipe(text))\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    123\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    157\u001b[0m     \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/pipelines/base.py:1162\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1155\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1156\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[1;32m   1161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1162\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/pipelines/base.py:1169\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1168\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1169\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[1;32m   1170\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1171\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/pipelines/base.py:1068\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1067\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1068\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[1;32m   1069\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1070\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(model_forward)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    186\u001b[0m     model_inputs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1198\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1198\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroberta(\n\u001b[1;32m   1199\u001b[0m     input_ids,\n\u001b[1;32m   1200\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1201\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1202\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   1203\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m   1204\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1205\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1206\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1207\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1208\u001b[0m )\n\u001b[1;32m   1209\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    828\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    829\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    830\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    834\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    836\u001b[0m     embedding_output,\n\u001b[1;32m    837\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m    838\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m    839\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    840\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m    841\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m    842\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    843\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    844\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    845\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    514\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    525\u001b[0m         hidden_states,\n\u001b[1;32m    526\u001b[0m         attention_mask,\n\u001b[1;32m    527\u001b[0m         layer_head_mask,\n\u001b[1;32m    528\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    529\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    530\u001b[0m         past_key_value,\n\u001b[1;32m    531\u001b[0m         output_attentions,\n\u001b[1;32m    532\u001b[0m     )\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:455\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    452\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    453\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 455\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    456\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward_chunk, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_size_feed_forward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len_dim, attention_output\n\u001b[1;32m    457\u001b[0m )\n\u001b[1;32m    458\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    460\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 236\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39minput_tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:468\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    467\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 468\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    469\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:379\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 379\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    380\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    381\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input = human_it \n",
    "loss_ = []\n",
    "prediction =[]\n",
    "for text in input:\n",
    "    text = text[:512]\n",
    "    prediction.append(pipe(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m \u001b[39minput\u001b[39m:\n\u001b[1;32m      5\u001b[0m     text \u001b[39m=\u001b[39m text[:\u001b[39m512\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m     prediction\u001b[39m.\u001b[39mappend(pipe(text))\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    123\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    157\u001b[0m     \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/pipelines/base.py:1162\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1155\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1156\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[1;32m   1161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1162\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/pipelines/base.py:1169\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1168\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1169\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[1;32m   1170\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1171\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/pipelines/base.py:1068\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1067\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1068\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[1;32m   1069\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1070\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(model_forward)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    186\u001b[0m     model_inputs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1198\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1198\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroberta(\n\u001b[1;32m   1199\u001b[0m     input_ids,\n\u001b[1;32m   1200\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1201\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1202\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   1203\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m   1204\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1205\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1206\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1207\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1208\u001b[0m )\n\u001b[1;32m   1209\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    828\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    829\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    830\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    834\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    836\u001b[0m     embedding_output,\n\u001b[1;32m    837\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m    838\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m    839\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    840\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m    841\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m    842\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    843\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    844\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    845\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    514\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    525\u001b[0m         hidden_states,\n\u001b[1;32m    526\u001b[0m         attention_mask,\n\u001b[1;32m    527\u001b[0m         layer_head_mask,\n\u001b[1;32m    528\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    529\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    530\u001b[0m         past_key_value,\n\u001b[1;32m    531\u001b[0m         output_attentions,\n\u001b[1;32m    532\u001b[0m     )\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\n\u001b[1;32m    414\u001b[0m         hidden_states,\n\u001b[1;32m    415\u001b[0m         attention_mask,\n\u001b[1;32m    416\u001b[0m         head_mask,\n\u001b[1;32m    417\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    418\u001b[0m         past_key_value\u001b[39m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    419\u001b[0m     )\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 340\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself(\n\u001b[1;32m    341\u001b[0m         hidden_states,\n\u001b[1;32m    342\u001b[0m         attention_mask,\n\u001b[1;32m    343\u001b[0m         head_mask,\n\u001b[1;32m    344\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    345\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    346\u001b[0m         past_key_value,\n\u001b[1;32m    347\u001b[0m         output_attentions,\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    350\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:266\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    263\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n\u001b[1;32m    265\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    268\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ADS/lib/python3.11/site-packages/torch/nn/functional.py:1828\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1824\u001b[0m         ret \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39m\u001b[39minput\u001b[39m)\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m   1825\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n\u001b[0;32m-> 1828\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax\u001b[39m(\u001b[39minput\u001b[39m: Tensor, dim: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, _stacklevel: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, dtype: Optional[DType] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m   1829\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Apply a softmax function.\u001b[39;00m\n\u001b[1;32m   1830\u001b[0m \n\u001b[1;32m   1831\u001b[0m \u001b[39m    Softmax is defined as:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1851\u001b[0m \n\u001b[1;32m   1852\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input = human_it_tr\n",
    "loss_ = []\n",
    "prediction =[]\n",
    "for text in input:\n",
    "    text = text[:512]\n",
    "    prediction.append(pipe(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = human_de_tr\n",
    "loss_ = []\n",
    "prediction =[]\n",
    "for text in input:\n",
    "    text = text[:512]\n",
    "    prediction.append(pipe(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999037504196167"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_file_path = \"Roberta_h_de.csv\"\n",
    "\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile: \n",
    "    # Write headers if needed\n",
    "    # writer.writerow(['Column1', 'Column2', ...])\n",
    "    \n",
    "    # Write data rows\n",
    "    for item in prediction:\n",
    "        csvfile.write(str(item[0])+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
